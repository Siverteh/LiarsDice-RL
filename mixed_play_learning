"""
Mixed self-play and curriculum learning for reinforcement learning agents in Liar's Dice.

This module implements a hybrid approach that combines self-play with curriculum learning
to create more robust agents that perform well against both themselves and rule-based opponents.
"""

import os
import time
import torch
import numpy as np
import matplotlib.pyplot as plt
import datetime
import json
import random
from typing import List, Dict, Any, Optional, Union, Tuple
from collections import deque

from agents.base_agent import RLAgent
from agents.agent_factory import create_agent
from agents.rule_agent import CURRICULUM_LEVELS
from environment.game import LiarsDiceGame
from environment.state import ObservationEncoder
from training.environment_wrapper import LiarsDiceEnvWrapper
from training.train import train_agent, evaluate_agent, train_episode
from training.evaluate import evaluate_against_curriculum, visualize_evaluation_results
from training.utils import (
    setup_logger, 
    save_training_data, 
    load_training_data,
    plot_training_results
)


def mixed_self_play_curriculum(
    # Base settings
    model_path: str,
    results_path: str = 'results/mixed_training',
    seed: Optional[int] = None,
    device: str = 'auto',
    render_training: bool = False,
    
    # Game setup
    num_players: int = 2,
    num_dice: int = 3,
    dice_faces: int = 6,
    
    # Training parameters
    num_rounds: int = 5,
    episodes_per_round: int = 10000,
    self_play_ratio: float = 0.7,  # 70% self-play, 30% curriculum
    win_rate_threshold: float = 0.85,
    opponent_update_frequency: int = 1000,
    
    # Population-based parameters
    use_population: bool = True,
    population_size: int = 5,
    
    # Agent configuration
    custom_agent_config: Optional[Dict[str, Any]] = None,
    
    # Training control
    checkpoint_frequency: int = 500,
    evaluation_frequency: int = 100,
    
    # Exploration settings
    initial_epsilon: float = 0.3,
    min_epsilon: float = 0.05,
    epsilon_decay_per_round: float = 0.5
) -> Tuple[RLAgent, Dict[str, Any]]:
    """
    Train an agent using a mixed approach of self-play and curriculum learning.
    
    Args:
        model_path: Path to the pre-trained model to load
        results_path: Directory to save results, checkpoints, and logs
        seed: Random seed for reproducibility
        device: Device to run on ('cpu', 'cuda', or 'auto')
        render_training: Whether to render gameplay during training
        
        # Game settings
        num_players: Number of players in the game
        num_dice: Number of dice per player
        dice_faces: Number of faces on each die
        
        # Training parameters
        num_rounds: Number of training rounds
        episodes_per_round: Number of episodes per round
        self_play_ratio: Ratio of self-play to curriculum learning (0.7 = 70% self-play)
        win_rate_threshold: Win rate threshold to advance rounds
        opponent_update_frequency: How often to update opponent model (in episodes)
        
        # Population-based parameters
        use_population: Whether to use population-based training
        population_size: Size of the model population
        
        # Agent configuration
        custom_agent_config: Additional agent-specific parameters
        
        # Training control
        checkpoint_frequency: How often to save checkpoints (in episodes)
        evaluation_frequency: How often to evaluate the agent (in episodes)
        
        # Exploration settings
        initial_epsilon: Starting exploration rate (for DQN)
        min_epsilon: Minimum exploration rate
        epsilon_decay_per_round: How much to reduce epsilon per round
        
    Returns:
        Tuple of (trained_agent, training_results)
    """
    # Load model metadata to get agent type and configuration
    if os.path.isdir(model_path):
        metadata_path = os.path.join(model_path, 'metadata.json')
    else:
        metadata_path = os.path.join(os.path.dirname(model_path), 'metadata.json')
    
    if os.path.exists(metadata_path):
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)
        
        agent_type = metadata.get('agent_type', 'unknown')
        model_num_players = metadata.get('num_players', num_players)
        model_num_dice = metadata.get('num_dice', num_dice)
        model_network_size = metadata.get('network_size', [256, 128, 64])
    else:
        # Default values if metadata not found
        agent_type = 'unknown'
        model_network_size = [256, 128, 64]
        print(f"Warning: No metadata found for model. Assuming default configuration.")
    
    # Set up paths for results
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    run_name = f"mixed_{agent_type}_{num_players}p{num_dice}d_{timestamp}"
    base_path = os.path.join(results_path, run_name)
    checkpoint_dir = os.path.join(base_path, 'checkpoints')
    log_dir = os.path.join(base_path, 'logs')
    
    # Create directories
    os.makedirs(base_path, exist_ok=True)
    os.makedirs(checkpoint_dir, exist_ok=True)
    os.makedirs(log_dir, exist_ok=True)
    
    # Set up logging
    logger = setup_logger('mixed_training', os.path.join(log_dir, 'mixed_training.log'))
    logger.info(f"Starting mixed self-play and curriculum learning for Liar's Dice with loaded {agent_type} agent")
    logger.info(f"Game setup: {num_players} players, {num_dice} dice, {dice_faces} faces")
    logger.info(f"Training: {num_rounds} rounds with {episodes_per_round} episodes per round")
    logger.info(f"Self-play ratio: {self_play_ratio:.2f} (self-play: {self_play_ratio*100:.0f}%, curriculum: {(1-self_play_ratio)*100:.0f}%)")
    
    # Resolve device
    if device == 'auto':
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    logger.info(f"Using device: {device}")
    
    # Set random seeds for reproducibility
    if seed is not None:
        np.random.seed(seed)
        torch.manual_seed(seed)
        random.seed(seed)
        if device == 'cuda':
            torch.cuda.manual_seed(seed)
    
    # Create test environment to get dimensions
    env = LiarsDiceEnvWrapper(
        num_players=num_players,
        num_dice=num_dice,
        dice_faces=dice_faces,
        seed=seed
    )
    
    obs_shape = env.get_observation_shape()
    obs_dim = obs_shape[0]
    action_dim = env.get_action_dim()
    logger.info(f"Observation dimension: {obs_dim}, Action dimension: {action_dim}")
    
    # Create agent configuration
    if agent_type.lower() == 'dqn':
        agent_config = {
            'hidden_dims': model_network_size,
            'device': device,
            'epsilon_start': initial_epsilon,
            'epsilon_end': min_epsilon,
            'epsilon_decay': 0.9999  # Slower decay for more stable learning
        }
    elif agent_type.lower() == 'ppo':
        agent_config = {
            'hidden_dims': model_network_size,
            'device': device,
            'entropy_coef': 0.01,  # Lower entropy for more focused learning
            'update_frequency': 2048,
            'gae_lambda': 0.95
        }
    else:
        # Generic config for unknown agent type
        agent_config = {
            'hidden_dims': model_network_size,
            'device': device
        }
    
    # Apply any custom agent config
    if custom_agent_config:
        agent_config.update(custom_agent_config)
    
    # Create agent with the same architecture as the loaded model
    agent = create_agent(
        agent_type=agent_type,
        obs_dim=obs_dim,
        action_dim=action_dim,
        config=agent_config
    )
    
    # Set action mapping
    agent.set_action_mapping(env.action_mapping)
    
    # Load the pre-trained model
    logger.info(f"Loading pre-trained model from {model_path}")
    agent.load(model_path)
    
    # Create opponent agent for self-play (clone of main agent)
    opponent_agent = create_agent(
        agent_type=agent_type,
        obs_dim=obs_dim,
        action_dim=action_dim,
        config=agent_config
    )
    opponent_agent.set_action_mapping(env.action_mapping)
    opponent_agent.load(model_path)
    
    # If using population-based training, initialize the population
    model_population = []
    if use_population:
        logger.info(f"Initializing population-based training with size {population_size}")
        # Start with just the loaded model
        initial_model_path = os.path.join(checkpoint_dir, "population_initial")
        agent.save(initial_model_path)
        model_population.append({
            'path': initial_model_path,
            'win_rate': 0.0,  # Will be updated after first evaluation
            'agent': opponent_agent  # Reference to the agent object
        })
    
    # Create self-play environment
    self_play_env = LiarsDiceEnvWrapper(
        num_players=num_players,
        num_dice=num_dice,
        dice_faces=dice_faces,
        seed=seed,
        rl_agent_as_opponent=opponent_agent
    )
    
    # Create dictionary to track performance against rule-based agents
    rule_performance = {level: 0.5 for level in CURRICULUM_LEVELS}  # Default to 0.5 win rate
    
    # Function to select a rule-based opponent weighted by difficulty and performance
    def select_rule_opponent():
        # Invert win rates to give higher weights to opponents we struggle against
        weights = {level: max(0.01, 1.0 - rate) for level, rate in rule_performance.items()}
        
        # Normalize weights
        total_weight = sum(weights.values())
        normalized_weights = {level: w/total_weight for level, w in weights.items()}
        
        # Select weighted random opponent
        selection = random.random()
        cumulative = 0
        for level, weight in normalized_weights.items():
            cumulative += weight
            if selection <= cumulative:
                return level
        
        # Fallback
        return random.choice(CURRICULUM_LEVELS)
    
    # Function to create curriculum environment with selected opponent
    def create_curriculum_env(opponent_type):
        return LiarsDiceEnvWrapper(
            num_players=num_players,
            num_dice=num_dice,
            dice_faces=dice_faces,
            seed=seed,
            rule_agent_types=[opponent_type]
        )
    
    # Function to select an opponent from the population
    def select_population_opponent():
        if not model_population:
            return opponent_agent
            
        # Choose a random opponent from the population
        opponent_data = random.choice(model_population)
        return opponent_data['agent']
    
    # Track training progress and models
    all_results = {}
    current_epsilon = initial_epsilon
    best_model_path = os.path.join(checkpoint_dir, "best_model")
    best_winrate = 0.0
    
    # Save initial model
    agent.save(os.path.join(checkpoint_dir, "initial_model"))
    
    # Training rounds loop
    for round_idx in range(num_rounds):
        logger.info(f"\n=== Starting Mixed Training Round {round_idx+1}/{num_rounds} ===")
        
        # Set exploration rate for this round (DQN only)
        if agent_type.lower() == 'dqn':
            # Gradually reduce exploration as we progress through rounds
            if round_idx > 0:
                current_epsilon = max(min_epsilon, current_epsilon * epsilon_decay_per_round)
                agent.epsilon = current_epsilon
                logger.info(f"Setting epsilon to {current_epsilon:.3f} for round {round_idx+1}")
        
        # Create directory for this round
        round_dir = os.path.join(checkpoint_dir, f"round_{round_idx+1}")
        os.makedirs(round_dir, exist_ok=True)
        
        # Variables to track this round's progress
        round_best_winrate = 0.0
        round_best_model_path = None
        episodes_since_update = 0
        round_complete = False
        
        # Track when we should update the opponent
        episodes_since_opponent_update = 0
        
        # Training statistics
        self_play_episodes = 0
        curriculum_episodes = 0
        curriculum_opponents = {level: 0 for level in CURRICULUM_LEVELS}
        
        # Custom training loop
        episode = 0
        while episode < episodes_per_round and not round_complete:
            # Decide whether to do self-play or curriculum learning
            use_self_play = random.random() < self_play_ratio
            
            if use_self_play:
                # Use self-play environment
                current_env = self_play_env
                self_play_episodes += 1
                
                # Update opponent model periodically
                episodes_since_opponent_update += 1
                if episodes_since_opponent_update >= opponent_update_frequency:
                    # Save current agent model
                    temp_path = os.path.join(round_dir, f"temp_model_{episode}")
                    agent.save(temp_path)
                    
                    if use_population:
                        # Update a random opponent from the population or add a new one
                        if len(model_population) < population_size or random.random() < 0.3:
                            # Create a new opponent
                            new_opponent = create_agent(
                                agent_type=agent_type,
                                obs_dim=obs_dim,
                                action_dim=action_dim,
                                config=agent_config
                            )
                            new_opponent.set_action_mapping(env.action_mapping)
                            new_opponent.load(temp_path)
                            
                            # Add to population
                            model_population.append({
                                'path': temp_path,
                                'win_rate': 0.0,  # Will be updated after evaluation
                                'agent': new_opponent
                            })
                            
                            # If population is too large, remove the worst performer
                            if len(model_population) > population_size:
                                model_population.sort(key=lambda x: x['win_rate'])
                                model_population.pop(0)  # Remove worst model
                            
                            logger.info(f"Added new model to population (size: {len(model_population)})")
                        else:
                            # Update existing opponent
                            update_idx = random.randrange(len(model_population))
                            model_population[update_idx]['agent'].load(temp_path)
                            model_population[update_idx]['path'] = temp_path
                            logger.info(f"Updated population model {update_idx}")
                        
                        # Select an opponent for the next episodes
                        next_opponent = select_population_opponent()
                        self_play_env.update_rl_opponent(next_opponent)
                    else:
                        # Simple opponent update (no population)
                        opponent_agent.load(temp_path)
                        self_play_env.update_rl_opponent(opponent_agent)
                    
                    logger.info(f"Updated opponent model at episode {episode}")
                    episodes_since_opponent_update = 0
            else:
                # Create curriculum environment with selected opponent
                opponent_type = select_rule_opponent()
                current_env = create_curriculum_env(opponent_type)
                curriculum_episodes += 1
                curriculum_opponents[opponent_type] += 1
            
            # Run a training episode
            reward, steps, info = train_episode(
                current_env, agent, render=False, reward_shaping=True
            )
            
            # Update episode counter
            episode += 1
            
            # Periodically evaluate the agent
            if episode % evaluation_frequency == 0:
                # Evaluate against self-play
                self_play_wins = 0
                for _ in range(30):
                    eval_reward, _, _ = train_episode(self_play_env, agent, evaluation=True)
                    if eval_reward > 0:
                        self_play_wins += 1
                
                self_play_win_rate = self_play_wins / 30
                
                # Evaluate against curriculum
                curriculum_results = evaluate_against_curriculum(
                    agent=agent,
                    num_episodes_per_level=50,  # Fewer episodes for faster evaluation
                    num_players=num_players,
                    num_dice=num_dice,
                    dice_faces=dice_faces,
                    epsilon=0.05,
                    seed=seed,
                    verbose=False
                )
                
                # Update rule performance for opponent selection
                for level, result in curriculum_results.items():
                    if level != 'overall':
                        rule_performance[level] = result['win_rate']
                
                curriculum_win_rate = curriculum_results['overall']['win_rate']
                
                # Combined win rate (weighted by training ratio)
                combined_win_rate = (self_play_win_rate * self_play_ratio + 
                                     curriculum_win_rate * (1 - self_play_ratio))
                
                # Log evaluation results
                logger.info(f"Evaluation at episode {episode}:")
                logger.info(f"  Self-play win rate: {self_play_win_rate:.3f}")
                logger.info(f"  Curriculum win rate: {curriculum_win_rate:.3f}")
                logger.info(f"  Combined win rate: {combined_win_rate:.3f}")
                
                # Update best model for this round if better
                if combined_win_rate > round_best_winrate:
                    round_best_winrate = combined_win_rate
                    round_best_model_path = os.path.join(round_dir, f"best_model")
                    agent.save(round_best_model_path)
                    logger.info(f"New best model for round {round_idx+1}: {combined_win_rate:.3f} win rate")
                    episodes_since_update = 0
                else:
                    episodes_since_update += evaluation_frequency
                
                # Update best overall model if better
                if combined_win_rate > best_winrate:
                    best_winrate = combined_win_rate
                    best_model_path = os.path.join(checkpoint_dir, "best_overall")
                    agent.save(best_model_path)
                    logger.info(f"New best overall model: {combined_win_rate:.3f} win rate")
                
                # If using population, update win rates
                if use_population:
                    for i, model_data in enumerate(model_population):
                        # Load the model and evaluate
                        temp_agent = create_agent(
                            agent_type=agent_type,
                            obs_dim=obs_dim,
                            action_dim=action_dim,
                            config=agent_config
                        )
                        temp_agent.set_action_mapping(env.action_mapping)
                        temp_agent.load(model_data['path'])
                        
                        # Quick self-play evaluation
                        pop_wins = 0
                        for _ in range(20):
                            # Create a temporary environment with current agent as opponent
                            temp_env = LiarsDiceEnvWrapper(
                                num_players=num_players,
                                num_dice=num_dice,
                                dice_faces=dice_faces,
                                seed=seed,
                                rl_agent_as_opponent=agent
                            )
                            eval_reward, _, _ = train_episode(temp_env, temp_agent, evaluation=True)
                            if eval_reward > 0:
                                pop_wins += 1
                        
                        model_data['win_rate'] = pop_wins / 20
                    
                    # Log population stats - FIXED this line that was causing the error
                    win_rates = [round(m['win_rate'], 2) for m in model_population]
                    logger.info(f"Population win rates: {win_rates}")
                
                # Check if we've reached the win rate threshold
                if combined_win_rate >= win_rate_threshold:
                    logger.info(f"Win rate threshold {win_rate_threshold:.3f} reached with {combined_win_rate:.3f}")
                    round_complete = True
                
                # If we haven't improved in a while, also move to next round
                if episodes_since_update >= 5 * evaluation_frequency:
                    logger.info(f"No improvement for {episodes_since_update} episodes, moving to next round")
                    round_complete = True
            
            # Save checkpoint periodically
            if episode % checkpoint_frequency == 0:
                checkpoint_path = os.path.join(round_dir, f"checkpoint_{episode}")
                agent.save(checkpoint_path)
                logger.info(f"Saved checkpoint to {checkpoint_path}")
        
        # Round complete - log statistics
        logger.info(f"Completed mixed training round {round_idx+1}")
        logger.info(f"Episodes: {episode}/{episodes_per_round} (self-play: {self_play_episodes}, curriculum: {curriculum_episodes})")
        logger.info(f"Best win rate: {round_best_winrate:.3f}")
        
        # Log curriculum opponent distribution
        logger.info("Curriculum opponent distribution:")
        for opponent, count in curriculum_opponents.items():
            if count > 0:
                logger.info(f"  {opponent}: {count} episodes ({count/curriculum_episodes*100:.1f}%)")
        
        # Add round results
        all_results[f"round_{round_idx+1}"] = {
            "episodes": episode,
            "self_play_episodes": self_play_episodes,
            "curriculum_episodes": curriculum_episodes,
            "curriculum_distribution": curriculum_opponents,
            "final_win_rate": round_best_winrate,
            "model_path": round_best_model_path
        }
        
        # Use the best model from this round for the next round
        if round_best_model_path and os.path.exists(round_best_model_path):
            logger.info(f"Loading best model from round {round_idx+1} for next round")
            agent.load(round_best_model_path)
            
            # Also update opponents
            if use_population:
                # Add best model to population
                new_opponent = create_agent(
                    agent_type=agent_type,
                    obs_dim=obs_dim,
                    action_dim=action_dim,
                    config=agent_config
                )
                new_opponent.set_action_mapping(env.action_mapping)
                new_opponent.load(round_best_model_path)
                
                model_population.append({
                    'path': round_best_model_path,
                    'win_rate': round_best_winrate,
                    'agent': new_opponent
                })
                
                # Keep only the best models
                if len(model_population) > population_size:
                    model_population.sort(key=lambda x: x['win_rate'], reverse=True)
                    model_population = model_population[:population_size]
                
                # Select a random opponent for the next round
                next_opponent = select_population_opponent()
                self_play_env.update_rl_opponent(next_opponent)
            else:
                # Simple opponent update
                opponent_agent.load(round_best_model_path)
                self_play_env.update_rl_opponent(opponent_agent)
    
    # Final evaluation
    logger.info("\n=== Final Evaluation ===")
    final_eval_results = evaluate_against_curriculum(
        agent=agent,
        num_episodes_per_level=300,
        num_players=num_players,
        num_dice=num_dice,
        dice_faces=dice_faces,
        epsilon=0.05,
        seed=seed,
        verbose=True
    )
    
    # Visualize final evaluation
    vis_path = os.path.join(log_dir, "final_evaluation.png")
    visualize_evaluation_results(final_eval_results, save_path=vis_path)
    logger.info(f"Saved final evaluation visualization to {vis_path}")
    
    # Save combined training data
    combined_results = {
        'round_results': all_results,
        'final_evaluation': final_eval_results,
        'parameters': {
            'agent_type': agent_type,
            'num_players': num_players,
            'num_dice': num_dice,
            'dice_faces': dice_faces,
            'num_rounds': num_rounds,
            'episodes_per_round': episodes_per_round,
            'self_play_ratio': self_play_ratio,
            'use_population': use_population,
            'population_size': population_size,
            'win_rate_threshold': win_rate_threshold
        }
    }
    save_training_data(combined_results, os.path.join(log_dir, 'mixed_training_results.pkl'))
    
    # Save final model with metadata
    models_dir = os.path.join(results_path, 'models')
    os.makedirs(models_dir, exist_ok=True)
    
    # Create model name with performance info
    overall_win_rate = final_eval_results['overall']['win_rate']
    model_name = f"mixed_{agent_type}_{num_players}p{num_dice}d_{overall_win_rate:.2f}wr_{timestamp}"
    model_path = os.path.join(models_dir, model_name)
    agent.save(model_path)
    
    # Save model metadata
    model_metadata = {
        'timestamp': timestamp,
        'agent_type': agent_type,
        'num_players': num_players,
        'num_dice': num_dice,
        'dice_faces': dice_faces,
        'win_rates': {k: v['win_rate'] for k, v in final_eval_results.items() if k != 'overall'},
        'overall_win_rate': overall_win_rate,
        'network_size': model_network_size,
        'training_parameters': {
            'num_rounds': num_rounds,
            'episodes_per_round': episodes_per_round,
            'self_play_ratio': self_play_ratio,
            'use_population': use_population,
            'population_size': population_size,
            'win_rate_threshold': win_rate_threshold
        }
    }
    
    with open(os.path.join(model_path, 'metadata.json'), 'w') as f:
        json.dump(model_metadata, f, indent=2)
    
    logger.info(f"Saved final model to {model_path}")
    logger.info("\nMixed training completed!")
    
    return agent, combined_results


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Mixed self-play and curriculum learning for Liar's Dice")
    parser.add_argument('--model', type=str, required=True,
                        help='Path to pre-trained model to load')
    parser.add_argument('--path', type=str, default='mixed_training/ppo_3dice',
                        help='Base path for results')
    parser.add_argument('--rounds', type=int, default=5,
                        help='Number of training rounds')
    parser.add_argument('--episodes', type=int, default=10000,
                        help='Episodes per round')
    parser.add_argument('--players', type=int, default=2,
                        help='Number of players')
    parser.add_argument('--dice', type=int, default=3,
                        help='Number of dice per player')
    parser.add_argument('--self-play-ratio', type=float, default=0.7,
                        help='Ratio of self-play to curriculum training (0.7 = 70% self-play)')
    parser.add_argument('--population', action='store_true',
                        help='Use population-based training')
    parser.add_argument('--population-size', type=int, default=5,
                        help='Size of model population')
    parser.add_argument('--win-rate', type=float, default=0.85,
                        help='Win rate threshold to advance rounds')
    parser.add_argument('--seed', type=int, default=None,
                        help='Random seed')
    parser.add_argument('--render', action='store_true',
                        help='Enable rendering during training')
    
    args = parser.parse_args()
    
    # Run mixed training with parsed arguments
    mixed_self_play_curriculum(
        model_path=args.model,
        results_path=args.path,
        num_rounds=args.rounds,
        episodes_per_round=args.episodes,
        num_players=args.players,
        num_dice=args.dice,
        self_play_ratio=args.self_play_ratio,
        use_population=args.population,
        population_size=args.population_size,
        win_rate_threshold=args.win_rate,
        seed=args.seed,
        render_training=args.render
    )